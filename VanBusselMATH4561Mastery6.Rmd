---
title: "MATH4561 Mastery VI"
author: "Melissa Van Bussel"
date: "April 20, 2019"
output:
  pdf_document:
    highlight: zenburn
    latex_engine: lualatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(knitr)
library(kableExtra)
```

# Problem 1

A study was conducted to determine the effects of individual bathers on the fecal
and total coliform bacterial populations in water. The variables of interest were the time since the subject's last bath, the vigor of the subject's activity in the water, and the subject's sex. The experiments were perfomed in a 100-gallon polyethylene tub using dechlorinated tap water at 38 degrees Celsius. The bacterial contribution of each bather was determined by subtracting the bacterial concentration measured at 15 and 30 minutes from that measured initially. A replicated $2^3$ factorial design was used for
this experiment. (Note: Because the measurement of bacterial populations in water involves a dilution technique, the experimental errors do not have constant variance. Rather, the variation increases with the value of the mean.) Perform analysis using a logarithmic
transfomation of the data.

The variables of interest are:

- $x_1$: time since last bath, with levels 1-hour and 24-hours
- $x_2$: vigor of bathing activity, with levels Lethargic and Vigorous
- $x_3$: sex of bather, with levels Female and Male

The response variable is:

- $y_3$: Total coliform contribution after 15 minutes (organisms/100mL)

(a) Briefly explain why this is a $2^3$ factorial design.

(b) Calculate the factorial effects (main and interaction effects) on total coliform populations after 15 minutes. Interpret the main effect of $x_1$, and the interaction between $x_1$ and $x_3$. Ensure you model the logarithm of $y_3$.

```{r}
prb0506 <- read.table(file = "prb0506.dat",header = T)
```

# Problem 1: Solution

## 1 (a)

This is referred to as a $2^3$ factorial design because we have 3 factors, each of which have two levels. The first factor is time since last bath (levels: 1 hour or 24 hours), the second factor is vigor of bathing activity (levels: lethargic or vigorous), and the third factor is sex of bather (levels: male or female). The number of treatment combinations is $2^3 = 8$, which is where the name comes from. 

## 1 (b)

To estimate the effects, we can use the `lm` function.

```{r}
bath <- as.factor(prb0506$x1)
vigor <- as.factor(prb0506$x2)
sex <- as.factor(prb0506$x3)
response <- log(prb0506$y3)
coliform <- data.frame(bath, vigor, sex, response)
mod <- lm(response ~ bath*vigor*sex, data = coliform)
summary(mod)
```

The "effects" are simply the coefficients of the terms in the model. From the summary output, we can see which effects are significant. However, a more visual representation can also help with the interpretation. Recall that if all coefficients are insignificant, then they should come from a Normal distribution with mean 0 due to the Central Limit Theorem. Therefore, by creating a Normal Q-Q plot, we can see which effects and interactions are significant. Coefficients which fall along the straight line are insignificant (come from a Normal Distribution), and outliers are significant (do not come from a Normal Distribution). 

```{r}
library(daewr)
fullnormal(coef(mod)[-1], alpha=.025)
```

We can see from the plot that many of the points deviate from the line, indicating significant effects. The only terms which are insignifcant are the interaction terms `bath*sex`, `vigor*sex` and `bath*vigor*sex`. 

We can further visualize the interaction effects by using interaction plots. For this question, we are interested in the interaction between `bath` and `sex`: 

```{r}
with(coliform, (interaction.plot(bath, sex, response, type = "b", pch =
c(18,24), main = "Interaction Plot",
xlab = "bath", ylab = "Total coliform after 15 minutes")))
```

From this interaction plot, we observe that there is no significant interaction, since the lines are approximately parallel. This tells us that, on average, changing the experimental unit from a man to a woman has a similar effect on the response (total coliform after 15 minutes) when the experimental unit has bathed an hour ago compared to when they bathed 24 hours ago. Changing the level of the `sex` variable does not change the effect that the `bath` variable has on the response, and vice versa.  

However, since we are also interested in the `bath` variable for this question, we need to consider any terms in the model which involve this variable, including interaction terms. If there are significant interaction terms which involve the `bath` variable, then the coefficient on the `bath` variable cannot be interpreted separately / on its own. 

Recall that the `bath*vigor` interaction term was significant in our model. Thus, we cannot directly interpret the `bath` variable's coefficient without considering other terms. We can, however, take a look at the coefficient of the `bath` variable and make a more general statement about it. First, we need to know which level of this factor had a higher average response. 

```{r}
bath_neg <- mean(subset(coliform, bath == -1, select = response)$response)
bath_pos <- mean(subset(coliform, bath == 1, select = response)$response)
bath_neg
bath_pos
```

We can see that the mean coliform level is higher for `bath = 1`. We aren't told whether `bath = 1` corresponds to 1 hour or 24 hours, but it seems reasonable to assume that bathing more recently would correspond to less bacteria, so let's make the assumption (for the purpose of answering this question) that `bath = 1` corresponds to having taken a bath 24 hours ago, while `bath = -1` corresponds to having taken a bath 1 hour ago. 

```{r, echo=FALSE}
coeff <- mod$coefficients[2]
```

**If we pretend for a moment that there was no significant interaction term involving the `bath` variable, we would interpret this coefficient in the following way:** if the other factors are kept constant (vigor and sex), then changing the `bath` variable from `-1` to `1` will increase the log total coliform contribution after 15 minutes by an average of `r coeff`. For example, if we were to take a measurement of the total coliform after 15 minutes from a man doing vigorous exercise who bathed an hour ago (and take the log of the measurement), and then repeat the experiment but have the man not shower for 24 hours beforehand, then we would expect the measurement of total coliform after 15 minutes to increase by roughly $e^{3.138322}$. However, this is a very small increase, and is essentially negligible, as this factor was found to be insignificant. While this factor does have *some* effect on the total coliform measurement (since the coefficient is not 0), the effect is so small that it is not worth including in our final model. We always desire the simplest model possible (the parsimonious model), so we do not include variables which are found to be insignificant. 

**However**, there is a significant interaction term involving the `bath` variable, so we cannot interpret the coefficient directly like that. Rather, we can interpret the coefficient as being significant, meaning that, on average, experimental units who bathe 24 hours ago will affect / increase the response more than those who bathed an hour ago. We can't directly interpret the value of the coefficient itself, though, because we'd need to take other terms in the model into consideration.

# Problem 2 

Le Riche and Csima (1964) evaluated four hypnotic drugs and a placebo to determine
their effect on the quality of sleep in elderly patients. The treatment levels were labeled (A = Placebo, B = Ethchlorvynol, C = Glutethimide, D = Chloral hydrate, and E = Secobarbitol sodium). Elderly patients were given one of the capsules for five nights in succession and their quality of sleep was rated by a trained nurse on a 4-point scale (0 = poor to 3 = excellent) each night. An average score was calculated for each patient over the five nights in a week. Each patient received all five treatments in successive weeks. A Latin-square design was used to account for patient-to-patient differences and week-to-week effects. The design and the response (mean quality of sleep rating) are shown in the table attached.

| Patient | 1        | 2        | 3        | 4        | 5        |
| :------ | :------- | :------- | :------- | :------- | :------- |
| 1       | B (2.92) | E (2.43) | A (2.19) | C (2.71) | D (2.71) |
| 2       | D (2.86) | A (1.64) | E (3.02) | B (3.03) | C (3.03) |
| 3       | E (1.97) | B (2.50) | C (2.47) | D (2.65) | A (1.89) |
| 4       | A (1.99) | C (2.39) | D (2.37) | E (2.33) | B (2.71) |
| 5       | C (2.64) | D (2.31) | B (2.44) | A (1.89) | E (2.78) |

(a) What is the appropriate model for this data?

(b) Complete the ANOVA and determine if there are any significant differences among the treatments.

(c) Use an appropriate method to determine if there is a significant difference between the placebo and the average of the other drugs, and if there are significant differences among the four drugs.

# Problem 2: Solution

## 2 (a)

In this situation, we are treating each patient as a block, and we are using a Latin Square Design to randomly assign the treatment order of the 5 treatments in each "block". Our column blocking factor is time, while our row blocking factor is the patient. Our 5 treatment levels are A = Placebo, B = Ethchlorvynol, C = Glutethimide, D = Chloral hydrate, and E = Secobarbitol sodium. Therefore, our model is: 

$$
y_{ijk} = r_i + c_j + \tau_k + \epsilon_{ijk}
$$

where $r_i, \; i = 1,2,3,4,5$ is the row blocking effect, $c_j, \; j = 1,2,3,4,5$ is the column-blocking effect, $\tau_k, \; k=1,2,3,4,5$ is the treatment effect, and $\epsilon_{ijk}$ is the experimental error term. 

## 2 (b)

First, we import the data. 

| Patient | 1        | 2        | 3        | 4        | 5        |
| :------ | :------- | :------- | :------- | :------- | :------- |
| 1       | B (2.92) | E (2.43) | A (2.19) | C (2.71) | D (2.71) |
| 2       | D (2.86) | A (1.64) | E (3.02) | B (3.03) | C (3.03) |
| 3       | E (1.97) | B (2.50) | C (2.47) | D (2.65) | A (1.89) |
| 4       | A (1.99) | C (2.39) | D (2.37) | E (2.33) | B (2.71) |
| 5       | C (2.64) | D (2.31) | B (2.44) | A (1.89) | E (2.78) |

```{r}
patient <- as.factor(rep(1:5, each = 5))
week <- as.factor(rep(1:5, 5))
p1t <- c("B", "E", "A", "C", "D") # patient 1 treatment order
p2t <- c("D", "A", "E", "B", "C")
p3t <- c("E", "B", "C", "D", "A")
p4t <- c("A", "C", "D", "E", "B")
p5t <- c("C", "D", "B", "A", "E")
treatment <- as.factor(c(p1t, p2t, p3t, p4t, p5t))
p1r <- c(2.92, 2.43, 2.19, 2.71, 2.71) # patient 1 response
p2r <- c(2.86, 1.64, 3.02, 3.03, 3.03)
p3r <- c(1.97, 2.50, 2.47, 2.65, 1.89)
p4r <- c(1.99, 2.39, 2.37, 2.33, 2.71)
p5r <- c(2.64, 2.31, 2.44, 1.89, 2.78)
response <- c(p1r, p2r, p3r, p4r, p5r)
sleep <- data.frame(patient, week, treatment, response)
head(sleep)
```

Now, we can get the ANOVA table: 

```{r}
mod <- aov(response ~ patient + week + treatment)
summary(mod)
```

We can see from the summary of the model that the `treatment` variable is significant at significance level $\alpha = 0.001$. Therefore, we have strong evidence that there are some differences between the different treatment levels. 

## 2 (c)

To determine if there is a significant difference between the placebo and the average of the other drugs, we can use the following contrast: $\tau_1 - \frac{1}{4}(\tau_2 + \tau_3 + \tau_4 + \tau_5)$. We can also use boxplots to visualize the data. 

```{r}
con1 <- c(1, -1/4, -1/4, -1/4, -1/4)
library(gmodels)
fit.contrast(mod, "treatment", con1)
boxplot(response ~ treatment, data = sleep)
```

From both the `fit.contrast` output and the boxplots, we can see that there is a significant difference in sleep quality between the placebo and the average of the other treatments. In fact, the other treatments significantly increase the patient's quality of sleep. 

To determine if there are significant differences between the four drugs, we will have to use the Tukey Honestly Significant Differences methods. If we were only comparing a couple of the drugs, we would be able to continue using contrats; however, if we were to do all of the pairwise combinations, we would quickly run into orthogonality issues. This is demonstrated below: 

```{r}
con2 <- c(0, 1, -1, 0, 0)
con3 <- c(0, 1, 0, -1, 0)
con4 <- c(0, 1, 0, 0, -1)
con5 <- c(0, 0, 1, -1, 0)
con6 <- c(0, 0, 1, 0, -1)
con7 <- c(0, 0, 0, 1, -1)
(con1 %*% con2)[1, 1]
(con1 %*% con3)[1, 1] 
(con1 %*% con4)[1, 1]
(con1 %*% con5)[1, 1] 
(con1 %*% con6)[1, 1]
(con1 %*% con7)[1, 1]
(con2 %*% con3)[1, 1]
(con2 %*% con4)[1, 1] 
(con2 %*% con5)[1, 1] 
(con2 %*% con6)[1, 1] 
(con2 %*% con7)[1, 1] 
(con3 %*% con4)[1, 1] 
(con3 %*% con5)[1, 1] 
(con3 %*% con6)[1, 1] 
(con3 %*% con6)[1, 1] 
(con4 %*% con5)[1, 1] 
(con4 %*% con6)[1, 1] 
(con4 %*% con7)[1, 1]
(con5 %*% con6)[1, 1] 
(con5 %*% con7)[1, 1]
(con6 %*% con7)[1, 1]
```

So, instead, we use the `TukeyHSD` function. 

```{r}
TukeyHSD(mod, which = "treatment")
```

From this, we can see that there is a significant difference in sleep quality between the placebo and each of the other treatments. There is no significant difference in sleep quality between any pairs of the drugs. Thus, so long as a patient takes one of the hypnotic drugs (doesn't matter which one), they should, on average, see an increase in their sleep quality. 

# Problem 3

A wooden catapult can be used to flip a foam ball. The catapult has three factors that can be adjusted: the start angle, the stop angle, and the pivot height. The distance the ball travels can be measured with a tape measure.

(a) If experiments were to be conducted with the catapult by flipping the ball and measuring the distance, what would the experimental unit be?

(b) Using the numbers 1, 2, and 3 to represent the levels of start angle and stop angle, and holding the pivot height constant at its high level, make a randomized list of experiments for a 3 X 3 factorial experiment with $r = 2$ replicates per cell.

# Problem 3: Solution

## 3 (a) 

**Recall, from Lawson**: *Experimental Unit* is the item under study upon which something is changed.

In our case, the item under study / the experimental unit is the distance of the catapulted ball, given the start angle, stop angle, and pivot height. 

## 3 (b) 

We are told to keep the pivot height constant, so we are only really varying two factors, each with 3 levels. Thus, we have a 3 x 3 factorial design. We need to use the `expand.grid` function on two vectors which each contain 3 levels, and then we can combine these results with a constant pivot height and a randomized order to obtain our final randomization plan.  

```{r}
start <- 1:3
stop <- 1:3 
combos <- expand.grid(start, stop)
start <- as.factor(rep(combos$Var1, 2))
stop <- as.factor(rep(combos$Var2, 2))
replicate <- rep(1:2, each = 9)
pivot <- as.factor(rep(3, 18))
order <- sample(1:18, 18, replace = FALSE)
catapult <- data.frame(start, stop, pivot, replicate, order)
kable(catapult, "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = "striped")
```

# Problem 4 

Professor Robert Wardrop, of the University of Wisconsin, conducted an experiment in 1991 to evaluate in which of five sound modes he best played a certain video game. The first three sound modes corresponded to three different types of background music, as well as game sounds expected to enhance play. The fourth mode had game sounds but no background music. The fifth mode had no music or game sounds. Denote these sound modes by the treatment factor levels 1-5, respectively. The experimenter observed that the game required no warmup, that boredom and fatigue would be a factor after 4 to 6 games, and that his performance varied considerably on a day-to-day basis. Hence, he used a Latin square design, with the two blocking factors being "day" and "time order of the game." The response measured was the game score, with higher scores being better. The design and resulting data are given in the attached table.

(a) Write down a possible model for these data and check the model assumptions. If the assumptions appear to be approximately satisfied, then answer parts (b)-(f).

(b) Plot the adjusted data and discuss the plot.

(c) Complete an analysis of variance table.

(d) Evaluate whether blocking was effective.

(e) Construct simultaneous 95% confidence intervals for all pairwise comparisons, as well as the "music versus no music" contrast: 

$$
\frac{1}{3}(\tau_1 + \tau_2 + \tau_3) - \frac{1}{2}(\tau_4 + \tau_5)
$$

and the "game sound versus no game sound" contrast:

$$
\frac{1}{4}(\tau_1 + \tau_2 + \tau_3 + \tau_4) - \tau_5
$$

(f) What are your conclusions from this experiment? Which sound mode(s) should Professor
Wardrop use?

# Problem 4: Solution 

## 4 (a)

### Model 

We are using a Latin Square design, with the row blocking factor the time order, and the column blocking factor the day. The treatment has 5 levels (music / game sounds, as described in the question). Our model is: 

$$
y_{ijk} = r_i + c_j + \tau_k + \epsilon_{ijk}
$$
where $r_i, \; i =1,2,3,4,5$ is the row block effect, $c_j, \; j=1,2,3,4,5$ is the column block effect, and $\tau_k, \;, k=1,2,3,4,5$ is the treatment effect. The final term in the model is for the experimental error. 

### Assumptions of the Model 

Before we can check the assumptions of the model, we must import the data.

```{r}
r1t <- c(1, 3, 4, 2, 5)
r2t <- c(3, 2, 1, 5, 4)
r3t <- c(4, 1, 5, 3, 2)
r4t <- c(5, 4, 2, 1, 3)
r5t <- c(2, 5, 3, 4, 1)
treatment <- as.factor(c(r1t, r2t, r3t, r4t, r5t))
r1r <- c(94, 100, 98, 101, 112)
r2r <- c(103, 111, 51, 110, 90)
r3r <- c(114, 75, 94, 85, 107)
r4r <- c(100, 74, 70, 93, 106)
r5r <- c(106, 95, 81, 90, 73)
response <- c(r1r, r2r, r3r, r4r, r5r)
day <- as.factor(rep(1:5, 5)) 
order <- as.factor(rep(1:5, each = 5))
game <- data.frame(day, order, treatment, response)
head(game)
```

Next, we fit the model: 

```{r}
mod <- aov(response ~ order + day + treatment, data = game)
summary(mod)
```

I'm pretty sure we didn't really talk about how to verify assumptions in class for the specific case of a latin square design, so I found a resource online that explains some tests that can be used (available at http://www.unh.edu/halelab/BIOL933/Labs/Lab5.pdf); latin squares are different than some of the other ones we covered in this course, since we need to ensure that the additive model works and we do not need interaction terms!

To check the assumption that errors are normally distributed, we plot a Normal Q-Q plot: 

```{r}
plot(mod, which = 2)
```

Since the points fall roughly on the straight line (appears that there is maybe one outlier at the bottom left), we are not concerned about this assumption not being met. 

Furthermore, we can plot the residuals and predicted values:

```{r}
resids <- residuals(mod)
preds <- predict(mod)
plot(resids ~ preds,
 xlab = "Predicted Values",
 ylab = "Residuals") 
```

There doesn't really appear to be any kind of pattern / trend, so we are not worried about this assumption not being met.

Finally, we can use the Shapiro-Wilk test to quantify the results we saw from the above plots.

```{r}
shapiro.test(resids)
```

Since the p-value is large, we fail to reject the null hypothesis. For the Shapiro-Wilk test, the null hypothesis is that the observations come from a Normal distribution. Therefore, we do not have significant evidence that the experimental errors are non-Normally distributed.

To verify the assumption that variances are homogeneous amongst treatment levels, we can perform Levene's Test: 

```{r}
library(car)
leveneTest(response ~ treatment, data = game)
```

Since the p-value is large, we fail to reject the null hypothesis (which is that the variances are homogeneous amongst treatment levels). Therefore, we do not have significant evidence that this assumption is not met. 

For a latin square design, we have an additive model. We therefore assume that the two blocking factors do not interact with each other, or with the treatment. We need to verify these assumptions, which we can do by using the Tukey 1-df test for Non-additivity (the following methodology is taken from the online resource I mentioned).

```{r}
# Testing for significance of the Row*Col interaction
row_column_mod <- lm(response ~ order + day, data = game)
row_column_preds <- predict(row_column_mod)
row_column_sqpreds <- row_column_preds^2
row_column_Tukey_mod <- lm(response ~ order + day + row_column_sqpreds, data = game)
anova(row_column_Tukey_mod)

# Testing for significance of the Treatment*Col interaction
trt_column_mod <- lm(response ~ treatment + day, data = game)
trt_column_preds <- predict(trt_column_mod)
trt_column_sqpreds <- trt_column_preds^2
trt_column_Tukey_mod <- lm(response ~ order + day + trt_column_sqpreds, data = game)
anova(trt_column_Tukey_mod)

# Testing for significance of the Row*Treatment interaction
row_trt_mod <- lm(response ~ order + treatment, data = game)
row_trt_preds <- predict(row_trt_mod)
row_trt_sqpreds <- row_trt_preds^2
row_trt_Tukey_mod <- lm(response ~ order + day + row_trt_sqpreds, data = game)
anova(row_trt_Tukey_mod)
```

Well, according to the resource I found online, the null hypothesis for the Tukey 1-df Non-additivity test is that there is no significant interaction. The p-value of interest for the test is the one in the bottomn right-hand corner. We can see that in 2 out of 3 of these tests, we have significant p-values (at varying significance levels), so we have significant evidence that there are some interactions. In particular, we see that the `treatment*day` and `treatment*order` interactions are significant. This means that the assumption of the latin square design (additive model) is actually *NOT* met. 

(I noticed afterwards that the `daewr` package actually has a function that does this, too. It's called `Tukey1df`.)

We can further explore this idea by using interaction plots, though these will be a bit difficult to interpret since there are 5 levels per factor. 

```{r}
with(game, (interaction.plot(order, day, response, type = "b", pch =
c(18,24), main = "Row*Column Interacton Plot",
xlab = "order", ylab = "Game Score")))
with(game, (interaction.plot(treatment, day, response, type = "b", pch =
c(18,24), main = "Treatment*Column Interacton Plot",
xlab = "treatment", ylab = "Game Score")))
with(game, (interaction.plot(order, treatment, response, type = "b", pch =
c(18,24), main = "Row*Treatment Interacton Plot",
xlab = "order", ylab = "Game Score")))
```

We can see that there is quite a lot of overlapping on all of the interaction plots, further confirming our suspicions that the assumptions of the model may not have been met. 
However, just in case I'm supposed to answer the question anyways / in case I verified the assumptions incorrectly, I will proceed.

## 4 (b)

We never talked about "adjusted data" in this course so I'll just assume that we are being asked to plot the response values along with the fitted values from the model. 

```{r}
plot(game$response, type = "l", ylim = c(10, 150), 
     main = "Game Sound Experiment",
     ylab = "Game Score")
lines(mod$fitted.values, type = "l", col = "red")
legend("topright",
       legend = c("response", "fitted"),
       col = c("black", "red"),
       lty = c(1, 1))
```

We can see that the fit is pretty good, but there area few spots where the model doesn't quite line up with the response. Some of the peaks and troughs do not match up, and there is a noticeable difference between the two lines. This might suggest that the fit is not very "good" -- in other words, the terms in the model may not be significant. In fact, if we recall the fit from earlier, not all three of the effects in the model were found to be significant. 

We can also do boxplots by factor: 

```{r}
boxplot(response ~ treatment, data = game)
```

We see from the boxplot that there appears to be a significant difference between some of the various treatment levels on Professor Wardrop's game score. 

It is also useful to look at boxplots of the response by the row blocking factor and column blocking factor. 

```{r}
boxplot(response ~ order, data = game)
boxplot(response ~ day, data = game)
```

Here, we don't see a significant difference between the game scores for the varying levels of the row blocking factor. For the column blocking factor, it is a much closer call, but it still doesn't look like there are significant differences between the game scores for the varying levels.

## 4 (c)

We have already outputted the ANOVA table; it is re-outputted here for convenience:

```{r}
summary(mod)
```

There are no significant effects in the model at significance level $\alpha = 0.05$, however, the `treatment` effect and the `day` effect are significant at the $\alpha = 0.10$ level. 

## 4 (d)

Blocking was only somewhat effective in this case. As there were no significant differences between levels of the row blocking factor, all that occurred by including this block in the model was unnecessary overcomplication of the model. Since this blocking factor did not actually add useful information to the model, it should not be included. The column blocking factor was a bit more significant, but still did not add a ton of value to the model. With this being said, however, since the interaction terms are being omitted (and since we found that there are actually significant interactions), it's possible that blocking would have been effective if some or all of the interaction terms were included. It is not possible to tell if this is the case, however, because there are not enough degrees of freedom to fit the model with all interaction terms.  

## 4 (e) 

To compute simultaneous 95% confidence intervals for pairwise comparisons, we use the `TukeyHSD` function (which defaults to a confidence level of 0.95): 

```{r}
TukeyHSD(mod, which = "treatment")
```

We can see that the only pairwise comparison which came back significant at the $\alpha = 0.05$ level is sound mode 5 compared to sound mode 1. We weren't given full details about the sound modes, but we do know that sound mode 5 corresponded to no game sounds with no music, while sound mode 1 corresponded to game sounds with music. Sound mode 5 was preferred over sound mode 1 in terms of increasing the professor's score. Perhaps in sound mode 1, the professor was becoming distracted by the music / sounds he was listening to. 

Aside from this one pair, we do not have evidence to suggest that any one treatment is better than any other in terms of improving the Professor's game score. This is basically what we would have expected, from our boxplots earlier. 

The 95% confidence intervals are given by the (lwr, upr) values in the output from the `TukeyHSD` function. The significant combination (sound mode 5 compared to sound mode 1) has a confidence interval which does not overlap 0, indicating its significance. Since all of the remaining intervals overlap 0, none of the remaining pairwise combinations have significant differences. 

Next, we are asked to compute the "music versus no music" contrast and the "game sound versus no game sound" contrast. 

First, we check orthogonality: 

```{r}
con1 <- c(1/3, 1/3, 1/3, -1/2, -1/2)
con2 <- c(1/4, 1/4, 1/4, 1/4, -1)
con1 %*% con2
```

We can see that the two contrasts are *not* orthogonal. For the purpose of completing the question, we continue as if they are. 

```{r}
fit.contrast(mod, "treatment", con1)
fit.contrast(mod, "treatment", con2)
```

For both contrasts, we have insignificant results (at significance level $\alpha = 0.05$). We see that the "game sound vs. no game sound" effect is significant at $\alpha = 0.10$, and that no game music is preferred. Thus, we do not have significant evidence to conclude that there is any difference in the Professor's game scores if he listens to music or not, but turning off his game sounds could increase his score (though we aren't really sure of this since the model assumptions were not met).

## 4 (f)

My conclusions are that the assumptions are not met, so the model is not really interpretable. The experiment should be re-run, with more data collected. Currently, there are not enough degrees of freedom to use other models, so it would be helpful to re-run the model, and determine which terms should be included in the model. 

Personally, I think that listening to game sounds should have a larger effect on a player's score (depending on the game obviously). This is because game sounds are there to allow the player to multitask most efficiently: having game sounds can tell the player valuable information about the game without the player needing to move their eyes around the screen as much and get distracted. I would actually think that game sounds should *increase* the player's score, not decrease it, but we aren't told what game it is, so who knows. 

From the results of this experiment alone, we cannot really provide recommendations to the Professor about which sound mode(s) he should use, since the model assumptions were not met. If the assumptions *were* met, we would recommend that he play this game with no game sounds, and that he listens to whichever background music he had for treatment level 1.

# Problem 6 

In an experiment to maximize the $Y$ = resolution of a peak on a gas chromatograph,
a significant interaction between $A$ = column temperature and $C$ = gas flow rate was found. The table below shows the mean resolution in each combination of column temperature and gas flow rate.

| Column Temp | Flow Low | Flow High |
| :---------- | :------- | :-------- |
| 120         | 10       | 13        |
| 180         | 12       | 18        |

(a) Construct an interaction graph.

(b) Write a sentence or two to interpret this interaction.

# Problem 6: Solution 

## 6 (a)

To be honest, the above table isn't set up very well -- I can't really tell if those numbers are all supposed to be mean resolutions, or if the "column temp" column is showing the two levels of column temperature. I'm going to assume it's the latter, because it doesn't really make sense otherwise. To make my assumption more clear, this is how I'm interpreting the above table: 

```{r}
temp <- as.factor(c(120, 180, 120, 180))
flow <- as.factor(c(rep("low", 2), rep("high", 2)))
resolution <- c(10, 12, 13, 18)
chromatograph <- data.frame(temp, flow, resolution)
kable(chromatograph, "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = "striped")
```

Now we can create the interaction plot: 

```{r}
with(chromatograph, (interaction.plot(temp, flow, resolution, type = "b", pch =
c(18,24), main = "Interaction Plot",
xlab = "temp", ylab = "Mean Resolution")))
```

## 6 (b)

We can see that there is a significant interaction between the `temp` and `flow` variables, since the slopes are not parallel. From the interaction plot, it can be observed that when the gas flow rate is low, changing the column temperature from 120 to 180 causes a small increase (2 units) in the mean resolution of the peak; however, when the gas flow rate is high, changing the column temperature from 120 to 180 causes a much larger increase (5 units) in the mean resolution of the peak on the chromatograph. 

I don't really remember everything from physics, but I'm pretty sure that having higher resolution is a good thing, because it's easier to detect where the actual peak is (?). If that's the case, then we can use this interaction plot to recommend that the scientists use a high gas flow rate and high column temperature (if these were the only two variables at play). A quick Google search reveals that it is indeed recommended to use a high gas flow rate / high temperature to improve resolution in gas chromatography experiments.

